{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import Parallel, delayed, dump\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os\n",
    "import gc\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_data(filename):\n",
    "    # load dataset\n",
    "    exames_dict = loadmat(filename)\n",
    "    \n",
    "    target = exames_dict['target']\n",
    "    exames = exames_dict['exames']\n",
    "\n",
    "    # samples as the first dimension\n",
    "    exames = np.transpose(exames,(2,0,1))\n",
    "\n",
    "    target = target[0]\n",
    "    return exames, target\n",
    "\n",
    "def filt_data(data, order, fs, f1, f2, band):\n",
    "    nyq = 0.5*fs  # nyquist frequency\n",
    "\n",
    "    a1,b1 = butter(order,[(f1-(band/2))/nyq,(f1+(band/2))/nyq],btype='bandpass')\n",
    "    a2,b2 = butter(order,[(f2-(band/2))/nyq,(f2+(band/2))/nyq],btype='bandpass')\n",
    "\n",
    "    dataf1 = filtfilt(a1,b1,data,axis=1)\n",
    "    dataf2 = filtfilt(a2,b2,data,axis=1)\n",
    "\n",
    "    return dataf1, dataf2\n",
    "\n",
    "def get_energy(data):\n",
    "    data = np.square(data)\n",
    "    return np.sum(data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons1=1, neurons2=0, n_features=None):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons1, input_dim=n_features, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    if neurons2 != 0:\n",
    "        model.add(Dense(neurons2, kernel_initializer='uniform', activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis function for parallel execution\n",
    "def cv_and_fit(params, train_index, test_index):\n",
    "\n",
    "    # mean zero and unit variance\n",
    "    scaler = StandardScaler()\n",
    "    train_data = scaler.fit_transform(features[train_index, :])\n",
    "    test_data = scaler.transform(features[test_index, :])\n",
    "    del scaler\n",
    "\n",
    "    # PCA\n",
    "    pca = PCA(n_components=params['npcs'])\n",
    "    train_data = pca.fit_transform(train_data)\n",
    "    test_data = pca.transform(test_data)\n",
    "    del pca\n",
    "\n",
    "    # normalization to [-1 1] range\n",
    "    scaler = MaxAbsScaler()\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    del scaler\n",
    "\n",
    "    # Multiple initiations maintaining the best result\n",
    "    best_acc = 0\n",
    "    #best_history = None\n",
    "    #best_model = None\n",
    "\n",
    "    for init in range(n_inits):\n",
    "\n",
    "        model = create_model(params['neurons1'], params['neurons2'], params['npcs'])\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=20, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(train_data, target[train_index], batch_size=25,\n",
    "                            validation_data=(test_data, target[test_index]),\n",
    "                            shuffle=True, epochs=2000, verbose=0, callbacks=[es])\n",
    "\n",
    "        # score of loss and accuracy for the model trained\n",
    "        score, acc = model.evaluate(test_data, target[test_index], batch_size=12, verbose=0)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            #best_history = history\n",
    "            #best_model = model\n",
    "\n",
    "        del history\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "    results_partial = {}\n",
    "    results_partial['npcs'] = params['npcs']\n",
    "    results_partial['neurons1'] = params['neurons1']\n",
    "    results_partial['neurons2'] = params['neurons2']\n",
    "    results_partial['acc'] = best_acc\n",
    "    #results_partial['model'] = best_model\n",
    "    #results_partial['history'] = best_history\n",
    "\n",
    "\n",
    "    #del best_history\n",
    "    #del best_model\n",
    "    gc.collect()\n",
    "\n",
    "    return results_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-3)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-3)]: Done  38 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-3)]: Done 188 tasks      | elapsed: 22.7min\n",
      "[Parallel(n_jobs=-3)]: Done 438 tasks      | elapsed: 58.0min\n",
      "[Parallel(n_jobs=-3)]: Done 788 tasks      | elapsed: 105.9min\n",
      "[Parallel(n_jobs=-3)]: Done 1100 out of 1100 | elapsed: 145.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Test_best_parameters']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = 601.5  # sampling rate\n",
    "f1 = 31.13  # modulating frequency 1\n",
    "f2 = 39.36  # modulating frequency 2\n",
    "\n",
    "band = 0.4\n",
    "order = 4\n",
    "\n",
    "win_size = 64\n",
    "nwin = 29\n",
    "\n",
    "n_jobs = -3\n",
    "n_folds = 10    # number of folds for k-fold\n",
    "n_inits = 10    # number of inits for each fold of k-fold\n",
    "\n",
    "neurons1 = list(range(11, 21))  # neurons 1st hidden layer\n",
    "neurons2 = list(range(0, 11))  # neurons 2nd hidden layer\n",
    "npcs = [16]  # num pcs to keep\n",
    "\n",
    "# grid with the parameters values\n",
    "param_grid = ParameterGrid({'npcs': npcs,\n",
    "                            'neurons1': neurons1,\n",
    "                            'neurons2': neurons2})\n",
    "\n",
    "\n",
    "# Start Analysis\n",
    "exames, target = load_data('/home/pedrosergiot/Documents/Exames2_sem1segundo.mat')\n",
    "\n",
    "dataf1, dataf2 = filt_data(exames[:,0:29,:], order, fs, f1, f2, band)\n",
    "\n",
    "energyf1 = get_energy(dataf1)\n",
    "energyf2 = get_energy(dataf2)\n",
    "\n",
    "# concatenates the energy values obtained\n",
    "features = np.concatenate((energyf1, energyf2), axis=1)\n",
    "\n",
    "# CV K-Fold\n",
    "kf = KFold(n_splits=n_folds)\n",
    "\n",
    "# Executing analysis in parallel\n",
    "parallel = Parallel(n_jobs=n_jobs, verbose=1, backend='loky')\n",
    "out = parallel(delayed(cv_and_fit)(params, train_index, test_index) \n",
    "               for params, (train_index, test_index) in product(param_grid, kf.split(features)))\n",
    "\n",
    "\n",
    "# Saving results for a given number of windows\n",
    "file_name = \"Test_best_parameters\"\n",
    "dump(out, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6321428537368774, 0.09008040939459754, (16, 19, 5)]\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(out)\n",
    "    \n",
    "gf = results_df.groupby(['npcs','neurons1','neurons2'])    \n",
    "max_mean1 = 0\n",
    "    \n",
    "for key in gf.groups.keys():        \n",
    "    if gf.get_group(key)['acc'].mean() > max_mean1:\n",
    "        max_mean1 = gf.get_group(key)['acc'].mean()\n",
    "        max_std1 = gf.get_group(key)['acc'].std()\n",
    "        max_key1 = key\n",
    "    \n",
    "print([max_mean1, max_std1, max_key1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
